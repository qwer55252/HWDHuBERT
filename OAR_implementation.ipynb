{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On Attention Redundancy 논문 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import 및 함수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Config, Wav2Vec2Processor\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# =============== #\n",
    "# 1. 모델 로드 예시\n",
    "# =============== #\n",
    "# 요청에 따라 Wav2Vec2 모델과 프로세서를 예시로 로드합니다.\n",
    "# (실제로는 BERT에서 attention을 추출해야 하므로, BERT 모델을 사용해야 함)\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "\n",
    "def cosine_distance(vec1, vec2):\n",
    "    \"\"\"1 - cosine similarity\"\"\"\n",
    "    # scipy의 cosine()는 distance를 바로 반환하므로 그대로 사용해도 됨\n",
    "    # distance = 1 - cosine_similarity => distance = cosine_distance\n",
    "    return cosine(vec1, vec2)\n",
    "\n",
    "def pearson_distance(vec1, vec2):\n",
    "    \"\"\"1 - Pearson correlation\"\"\"\n",
    "    # stats.pearsonr 리턴값: (상관계수, p-value)\n",
    "    corr, _ = pearsonr(vec1, vec2)\n",
    "    # corr 범위는 -1 ~ 1, 거리로 쓰려면 [0,2] 범위가 되지만\n",
    "    # 논문에서 0~1로 표준화한다고 했으니 1 - (corr+1)/2 로 매핑하는 식도 고려할 수 있음\n",
    "    # 여기서는 간단히 1 - corr로만 사용(음의 상관도 처리 방법은 연구 맥락에 따라 달라짐)\n",
    "    return 1 - corr\n",
    "\n",
    "def jensen_shannon_distance(p, q):\n",
    "    \"\"\"Jensen-Shannon distance (보통 JS divergence의 제곱근을 distance로 사용)\"\"\"\n",
    "    # 두 확률 분포 p, q가 들어온다고 가정 (합이 1)\n",
    "    p = np.array(p, dtype=np.float64)\n",
    "    q = np.array(q, dtype=np.float64)\n",
    "    p /= p.sum()\n",
    "    q /= q.sum()\n",
    "    m = 0.5 * (p + q)\n",
    "    # KL 계산 시 log(0) 방지를 위해 epsilon 추가 등 처리 가능\n",
    "    def kl_div(a, b):\n",
    "        a = np.where(a == 0, 1e-12, a)\n",
    "        b = np.where(b == 0, 1e-12, b)\n",
    "        return np.sum(a * np.log(a / b))\n",
    "    js_div = 0.5 * kl_div(p, m) + 0.5 * kl_div(q, m)\n",
    "    return np.sqrt(js_div)  # JS distance\n",
    "\n",
    "def bhattacharyya_distance(p, q):\n",
    "    \"\"\"Bhattacharyya distance\"\"\"\n",
    "    p = np.array(p, dtype=np.float64)\n",
    "    q = np.array(q, dtype=np.float64)\n",
    "    p /= p.sum()\n",
    "    q /= q.sum()\n",
    "    bc = np.sum(np.sqrt(p * q))\n",
    "    # Bhattacharyya distance = -ln(bc)\n",
    "    # 보통 0~∞ 범위, 논문에서는 [0,1] 범위로 정규화해서 사용\n",
    "    # 여기서는 간단히 -ln(bc)만 반환, 필요 시 max/min 스케일링 가능\n",
    "    return -np.log(bc + 1e-12)\n",
    "\n",
    "def get_token_based_distance(matA, matB, metric=\"cosine\"):\n",
    "    \"\"\"\n",
    "    matA, matB shape = (seq_len, seq_len)\n",
    "    각각의 '행(row)'을 해당 토큰(i)이 바라보는 attention distribution이라 가정.\n",
    "    토큰 i마다 vecA = matA[i, :], vecB = matB[i, :]\n",
    "    -> 두 벡터 간 distance 측정 -> 전체 토큰에 대해 평균.\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "    for i in range(matA.shape[0]):\n",
    "        vecA = matA[i, :]\n",
    "        vecB = matB[i, :]\n",
    "\n",
    "        if metric == \"cosine\":\n",
    "            dist = cosine_distance(vecA, vecB)\n",
    "        elif metric == \"corr\":\n",
    "            dist = pearson_distance(vecA, vecB)\n",
    "        elif metric == \"js\":\n",
    "            dist = jensen_shannon_distance(vecA, vecB)\n",
    "        elif metric == \"bc\":\n",
    "            dist = bhattacharyya_distance(vecA, vecB)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown token-based metric\")\n",
    "        \n",
    "        distances.append(dist)\n",
    "\n",
    "    return np.mean(distances)\n",
    "\n",
    "\n",
    "# =============== #\n",
    "# 4. Sentence-based distance 함수들\n",
    "# =============== #\n",
    "# 두 헤드의 n×n Attention matrix를 “전체 행렬” 단위로 비교해 거리를 구합니다.\n",
    "# 실제 논문에서 쓰는 distance correlation, Procrustes, Canonical correlation은\n",
    "# 구현이 조금 복잡하거나 추가 라이브러리가 필요합니다.\n",
    "# 여기서는 간단한 버전/흉내를 보여드립니다.\n",
    "\n",
    "def distance_correlation(A, B):\n",
    "    \"\"\"\n",
    "    Distance correlation(Szekely 등)은 임의 차원 행렬 간 독립성/종속성을\n",
    "    측정하기 위한 지표. 여기서는 간단화를 위해 행렬을 1D로 펼쳐서\n",
    "    pairwise distance 기반 계산을 아주 간단히 흉내만 낸 예시입니다.\n",
    "    실제 구현은 더 복잡한 절차가 필요합니다.\n",
    "    \"\"\"\n",
    "    # 실제론 pairwise distance행렬 a_{ij}, b_{ij}를 만들어\n",
    "    # double-centering 등을 거쳐 계산함.\n",
    "    # 여기서는 예시로 임시 변환(행렬->1D벡터) 후 상관계수로 대체.\n",
    "    A_flat = A.flatten()\n",
    "    B_flat = B.flatten()\n",
    "    corr, _ = pearsonr(A_flat, B_flat)\n",
    "    # distance = 1 - |corr|\n",
    "    # (실제 distance correlation과 동일하진 않지만, 예시로서...)\n",
    "    return 1 - abs(corr)\n",
    "\n",
    "def procrustes_distance(A, B):\n",
    "    \"\"\"\n",
    "    Procrustes analysis:\n",
    "    한 행렬을 회전/축척/직교변환하여 다른 행렬과 얼마나 잘 align되는지 보는 기법.\n",
    "    실제론 scipy의 procrustes 등을 사용할 수 있음.\n",
    "    여기서는 매우 단순화된 예시(행렬을 정규화해 차이만 보는 형태).\n",
    "    \"\"\"\n",
    "    # 간단히 frobenius norm으로 차이를 보는 예시\n",
    "    # 실제론 회전/직교 변환 등을 최적화로 구해야 함.\n",
    "    A_norm = A / (np.linalg.norm(A) + 1e-12)\n",
    "    B_norm = B / (np.linalg.norm(B) + 1e-12)\n",
    "    fro_diff = np.linalg.norm(A_norm - B_norm)\n",
    "    # fro_diff 범위가 [0,2] 정도 되므로, 0~1 사이로 스케일링하기 위해 /2\n",
    "    return fro_diff / 2.0\n",
    "\n",
    "def canonical_correlation_distance(A, B):\n",
    "    \"\"\"\n",
    "    Canonical Correlation: 두 데이터 집합(행렬)의 선형 조합들이\n",
    "    얼마나 상관관계가 높은지 측정하는 기법.\n",
    "    실제론 차원 맞춤, SVD 등을 이용해야 함.\n",
    "    여기서는 간단히 SVD 없이 열벡터를 이어 붙인 후 상관관계로 예시 대체.\n",
    "    \"\"\"\n",
    "    # 예시로 (seq_len x seq_len)을 (seq_len^2)로 펴서 상관 계산\n",
    "    A_flat = A.flatten()\n",
    "    B_flat = B.flatten()\n",
    "    corr, _ = pearsonr(A_flat, B_flat)\n",
    "    # corr이 높을수록 유사 -> distance = 1 - corr\n",
    "    return 1 - corr\n",
    "\n",
    "def get_sentence_based_distance(matA, matB, metric=\"dCor\"):\n",
    "    \"\"\"\n",
    "    matA, matB shape = (seq_len, seq_len)\n",
    "    한 문장 전체의 attention 행렬을 직접 비교.\n",
    "    \"\"\"\n",
    "    if metric == \"dCor\":\n",
    "        return distance_correlation(matA, matB)\n",
    "    elif metric == \"PC\":\n",
    "        return procrustes_distance(matA, matB)\n",
    "    elif metric == \"CC\":\n",
    "        return canonical_correlation_distance(matA, matB)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown sentence-based metric\")\n",
    "\n",
    "\n",
    "# =============== #\n",
    "# 5. 실제 Pairwise Distance 계산 예시\n",
    "# =============== #\n",
    "# 여기서는 head가 4개이므로 4x4=16개 쌍에 대해 거리를 구하여\n",
    "# (4,4) 크기의 distance matrix를 얻을 수 있습니다.\n",
    "\n",
    "def compute_pairwise_distances(attention_mats, distance_func, mode=\"token\", metric=\"cosine\"):\n",
    "    \"\"\"\n",
    "    attention_mats: shape = (num_heads, seq_len, seq_len)\n",
    "    distance_func: get_token_based_distance() or get_sentence_based_distance()\n",
    "    mode: \"token\" or \"sentence\"\n",
    "    metric: 사용할 세부 지표\n",
    "    \"\"\"\n",
    "    num_heads = attention_mats.shape[0]\n",
    "    dist_matrix = np.zeros((num_heads, num_heads))\n",
    "\n",
    "    for i in range(num_heads):\n",
    "        for j in range(num_heads):\n",
    "            if i == j:\n",
    "                dist_matrix[i, j] = 0.0\n",
    "            elif j > i:\n",
    "                dist_val = distance_func(attention_mats[i], attention_mats[j], metric)\n",
    "                dist_matrix[i, j] = dist_val\n",
    "                dist_matrix[j, i] = dist_val\n",
    "            # j < i인 경우 이미 계산됨\n",
    "\n",
    "    return dist_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 랜덤 파형 입력 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예: 샘플 레이트 16k, 1초짜리 랜덤 파형 1개(batch_size=1)\n",
    "# TODO: 데이터 수 늘려야함 + 전체적인 하이퍼파라미터 수정 필요\n",
    "\n",
    "sample_rate = 16000\n",
    "duration = 1 # seconds\n",
    "batch_size = 1\n",
    "audio_tensor = torch.randn(batch_size, sample_rate * duration) # (B, T)\n",
    "audio_tensor.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 모델 선언 + Attention 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-100h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Wav2Vec2Model is using Wav2Vec2SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/wav2vec2-base-100h\"\n",
    "\n",
    "config = Wav2Vec2Config.from_pretrained(model_name, output_attentions=True)\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name, config=config)\n",
    "model = Wav2Vec2Model.from_pretrained(model_name, config=config)\n",
    "\n",
    "# forward 시, attention을 함께 받아오기\n",
    "with torch.no_grad():\n",
    "    outputs = model(audio_tensor, output_attentions=True)\n",
    "    # outputs.attentions: 튜플 형태, (num_layers, B, num_heads, seq_len, seq_len)\n",
    "    attentions = outputs.attentions\n",
    "\n",
    "num_layers = len(attentions)                    # 12\n",
    "num_heads_per_layer = attentions[0].shape[1]    # 12\n",
    "total_heads = num_layers * num_heads_per_layer  # 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(attentions): <class 'tuple'>\n",
      "len(attentions) : 12\n",
      "attentions[0].shape: torch.Size([1, 12, 49, 49])\n"
     ]
    }
   ],
   "source": [
    "print(f'type(attentions): {type(attentions)}')\n",
    "print(f'len(attentions) : {len(attentions)}')\n",
    "print(f'attentions[0].shape: {attentions[0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Redundancy Matrix 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=1이므로 dim=0 없애기\n",
    "attention_matrices = torch.stack(attentions).squeeze(1) # torch.Size([12, 12, 104, 104])\n",
    "\n",
    "# (num_heads * num_layers, seq_len, seq_len)으로 변환\n",
    "seq_len = attention_matrices.shape[-1]\n",
    "attention_matrices = attention_matrices.view(-1, seq_len, seq_len) # torch.Size([144, 104, 104])\n",
    "                            \n",
    "head_distance_matrix = compute_pairwise_distances(\n",
    "    attention_matrices,\n",
    "    distance_func=get_token_based_distance,\n",
    "    mode=\"token\",\n",
    "    metric=\"corr\"\n",
    ")\n",
    "\n",
    "# TODO: head_distance_matirx를 0~1로 정규화 할 필요 있음 ✅\n",
    "head_distance_matrix = head_distance_matrix / 2.0 # head_distance_matrix를 0~1 범위로 정규화 (2로 나누기)\n",
    "head_similarity_matrix = 1.0 - head_distance_matrix \n",
    "\n",
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# imputer = SimpleImputer(strategy='mean')  # 또는 'median', 'most_frequent' 등\n",
    "# head_similarity_matrix_imputed = imputer.fit_transform(head_similarity_matrix)\n",
    "\n",
    "\n",
    "\n",
    "# Create a heatmap for the token cosine distance matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(head_distance_matrix, cmap='viridis', annot=False)\n",
    "plt.title(\"Token - Pearson Correlation Coefficient Matrix Heatmap\")\n",
    "plt.xlabel(\"Head Index\")\n",
    "plt.ylabel(\"Head Index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 클러스터링(SpectralClustering) + Silhouette Score 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스펙트럴 클러스터링을 위해 precomputed affinity 사용을 가정\n",
    "# SKlearn의 SpectralClustering을 사용하여 스펙트럴 클러스터링을 수행 -> 여기서는 유사도 행렬을 사용해야만 함\n",
    "\n",
    "pruning_ratio = 0.1\n",
    "# 원하는 클러스터 개수 (pruning_ratio 파라미터에 따라 결정되게 수정 필요)\n",
    "n_clusters = int(total_heads * pruning_ratio)\n",
    "clustering = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', assign_labels='kmeans', random_state=42)\n",
    "\n",
    "# 클러스터 레이블 예측 \n",
    "cluster_labels = clustering.fit_predict(head_similarity_matrix) # head_similarity_matrix는 (144x144) 헤드간의 유사도 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SpectralClustering(affinity=&#x27;precomputed&#x27;, n_clusters=14, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SpectralClustering<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.cluster.SpectralClustering.html\">?<span>Documentation for SpectralClustering</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SpectralClustering(affinity=&#x27;precomputed&#x27;, n_clusters=14, random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "SpectralClustering(affinity='precomputed', n_clusters=14, random_state=42)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(cluster_labels) : 144\n",
      "cluster_labels: [11 11  4  0  8 11  0  0  9  6 12 10  8  7 11  5  1  9  7  6  1  7 10  9\n",
      "  6  3  8 11  9  1 10 13 12 11 13  5  0 13  7  0  5  7 13 12  8  0 13 13\n",
      "  4  7  7  5  5  7  4  7  4  4  5  4  4 12  4  4  4  4  4  4  5  5  7  4\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  2  3  3  7  3  3  1  3  3  1\n",
      "  2  2  1  9  2  0  1  2  2  9  2  2  6  1  2  3  2  3  3  3  6  3  3  3]\n"
     ]
    }
   ],
   "source": [
    "print(f'len(cluster_labels) : {len(cluster_labels)}')\n",
    "print(f'cluster_labels: {cluster_labels}')\n",
    "# clustser_lables[i] = c -> i번재 Head의 Cluster label = c 라는 뜻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectral Clustering Silhouette Score: 0.1858\n"
     ]
    }
   ],
   "source": [
    "# Silhouette Score 계산\n",
    "# 여기서는 유사도 행렬이 아니라 \"거리 행렬\"을 사용해야함\n",
    "\n",
    "try:\n",
    "    # metric='precomputed'로 해서 head_redundancy_matrix 자체를 거리 행렬로 간주\n",
    "    score = silhouette_score(\n",
    "        head_distance_matrix,\n",
    "        cluster_labels,\n",
    "        metric='precomputed'\n",
    "    )\n",
    "    print(f\"Spectral Clustering Silhouette Score: {score:.4f}\")\n",
    "except Exception as e:\n",
    "    print(\"실루엣 스코어 계산 중 오류:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Pruning 대상 선정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\narray([12, 12,  0, 13,  7, 10,  9, 12, 11, 13,  7,  7, 12,  0,  2,  3,  9,\\n       11,  3, 11,  4,  0,  1, 11,  1, 10,  6,  4, 11,  7,  1, 13, 11, 12,\\n        2,  3,  6,  2,  0,  4,  3,  3,  6,  6,  6, 11,  2,  2,  1,  0,  0,\\n        3,  3,  1,  1,  3,  1,  1,  3,  1,  1,  4,  1,  1,  1,  1,  1,  1,\\n        3,  3,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  6,  0,  1,  1,\\n        1,  0,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\\n        1,  1,  1,  3,  1,  1,  1,  1,  3,  1,  1,  0,  1,  1,  1,  1,  1,\\n        1,  5,  3,  1,  5,  0,  5,  1,  3,  5, 10,  5,  5,  8,  9,  3,  1,\\n        0,  1,  1,  8,  8,  1,  1,  1], dtype=int32)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_labels\n",
    "'''\n",
    "array([12, 12,  0, 13,  7, 10,  9, 12, 11, 13,  7,  7, 12,  0,  2,  3,  9,\n",
    "       11,  3, 11,  4,  0,  1, 11,  1, 10,  6,  4, 11,  7,  1, 13, 11, 12,\n",
    "        2,  3,  6,  2,  0,  4,  3,  3,  6,  6,  6, 11,  2,  2,  1,  0,  0,\n",
    "        3,  3,  1,  1,  3,  1,  1,  3,  1,  1,  4,  1,  1,  1,  1,  1,  1,\n",
    "        3,  3,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  6,  0,  1,  1,\n",
    "        1,  0,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
    "        1,  1,  1,  3,  1,  1,  1,  1,  3,  1,  1,  0,  1,  1,  1,  1,  1,\n",
    "        1,  5,  3,  1,  5,  0,  5,  1,  3,  5, 10,  5,  5,  8,  9,  3,  1,\n",
    "        0,  1,  1,  8,  8,  1,  1,  1], dtype=int32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 클러스터별 대표 Head 1개만 남기고 나머지는 모두 제거\n",
    "# 예: 클러스터마다 첫 번째로 만나는 Head를 대표로 선정 <- 일단 이렇게 구현하자\n",
    "\n",
    "cluster_to_rep = {} # cluster_id -> 대표 head_id\n",
    "# TODO: 대표 head 선정 방법을 수정해야 함\n",
    "for head_idx, c_id in enumerate(cluster_labels):\n",
    "    if c_id not in cluster_to_rep:\n",
    "        cluster_to_rep[c_id] = head_idx\n",
    "# cluster_to_rep[c_id] = head_idx: c_id 클러스터의 대표 head는 head_idx번째 head\n",
    "\n",
    "# 각 클러스터 대표만 남긴다고 가정\n",
    "selected_head_indices = list(cluster_to_rep.values()) # values에는 대표 head index들이 들어있음\n",
    "selected_head_indices.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(selected_head_indices): 14\n",
      "selected_head_indices : [0, 2, 3, 4, 8, 9, 10, 11, 13, 15, 16, 25, 31, 110]\n",
      "[Before Pruning] Total Heads: 144\n",
      "[After Pruning ] Remaining Heads: 14\n"
     ]
    }
   ],
   "source": [
    "print(f'len(selected_head_indices): {len(selected_head_indices)}')\n",
    "print(f'selected_head_indices : {selected_head_indices}')\n",
    "# len(selected_head_indices): 28\n",
    "# selected_head_indices : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 27, 29, 30, 32, 36, 110, 132]\n",
    "# 이 Head 들만 남기고 나머지는 다 pruning\n",
    "\n",
    "print(f\"[Before Pruning] Total Heads: {total_heads}\")\n",
    "print(f\"[After Pruning ] Remaining Heads: {len(selected_head_indices)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0, 2, 3, 4, 8, 9, 10, 11],\n",
       " 1: [1, 3, 4],\n",
       " 2: [1, 7],\n",
       " 3: [],\n",
       " 4: [],\n",
       " 5: [],\n",
       " 6: [],\n",
       " 7: [],\n",
       " 8: [],\n",
       " 9: [2],\n",
       " 10: [],\n",
       " 11: []}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (선택된 head만 남겨두기 위해) 레이어 단위로 묶어본다\n",
    "layer_to_keep = {layer_idx: [] for layer_idx in range(num_layers)}\n",
    "\n",
    "for c_id, rep_head_idx in cluster_to_rep.items():\n",
    "    layer_id = rep_head_idx // num_heads_per_layer\n",
    "    head_id_in_layer = rep_head_idx % num_heads_per_layer\n",
    "    layer_to_keep[layer_id].append(head_id_in_layer)\n",
    "# layer_to_keep[layer_id] = [...] -> 해당 layer에서 남길 head들의 index들\n",
    "layer_to_keep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Pruning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prune_dict(model_config, layer_to_keep_dict):\n",
    "    \"\"\"\n",
    "    layer_to_keep_dict: 예) {0: [0, 2], 1: [1, 3, 5], ...}\n",
    "    model_config: model.config (Wav2Vec2Config)\n",
    "    \n",
    "    반환: heads_to_prune 형태의 dict\n",
    "       예) { layer_i: [prune_head_idx1, prune_head_idx2, ...], ... }\n",
    "    \"\"\"\n",
    "    num_attention_heads = model_config.num_attention_heads  # 보통 12\n",
    "    heads_to_prune = {}\n",
    "    \n",
    "    for layer_idx in range(model_config.num_hidden_layers):\n",
    "        keep_heads = set(layer_to_keep_dict.get(layer_idx, []))\n",
    "        all_heads = set(range(num_attention_heads))\n",
    "        # prune할 head는 all_heads - keep_heads\n",
    "        prune_heads = sorted(list(all_heads - keep_heads))\n",
    "        if prune_heads:\n",
    "            heads_to_prune[layer_idx] = prune_heads\n",
    "    \n",
    "    return heads_to_prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heads_to_prune= {0: [1, 5, 6, 7], 1: [0, 2, 5, 6, 7, 8, 9, 10, 11], 2: [0, 2, 3, 4, 5, 6, 8, 9, 10, 11], 3: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 4: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 5: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 6: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 7: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 8: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 9: [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11], 10: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 11: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace의 Wav2Vec2PreTrainedModel.prune_heads() 사용 예시\n",
    "# 내부적으로 base_model._prune_heads(heads_to_prune)를 호출해 실제 가중치 재구성\n",
    "heads_to_prune = build_prune_dict(model.config, layer_to_keep)\n",
    "print(\"heads_to_prune=\", heads_to_prune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pytorch_utils import prune_linear_layer\n",
    "\n",
    "def find_pruneable_heads_and_indices(heads, num_heads, head_size, already_pruned_heads=None):\n",
    "    \"\"\"\n",
    "    BERT 모델 pruning 로직을 참고해 작성한 유틸 함수.\n",
    "    `heads`: 제거할 head 번호들의 집합 (ex: {1, 3})\n",
    "    `num_heads`: 현재 레이어의 전체 head 수 (예: 12)\n",
    "    `head_size`: head당 dimension (예: 64)\n",
    "    `already_pruned_heads`: 이미 제거된 head들의 집합\n",
    "    \"\"\"\n",
    "    if already_pruned_heads is None:\n",
    "        already_pruned_heads = set()\n",
    "\n",
    "    # 현재까지 제거된 head들을 합집합으로\n",
    "    heads = set(heads) - already_pruned_heads\n",
    "    mask = torch.ones(num_heads, head_size)\n",
    "    # heads_to_prune에 해당하는 row는 0으로 만든다\n",
    "    for head in heads:\n",
    "        mask[head] = 0\n",
    "    mask = mask.view(-1).eq(1)\n",
    "    \n",
    "    index = torch.arange(num_heads * head_size)[mask]\n",
    "    return heads, index\n",
    "\n",
    "\n",
    "def prune_wav2vec2_attention_layer(attention_module, heads_to_prune):\n",
    "    \"\"\"\n",
    "    wav2vec2의 single layer(Wav2Vec2Attention)에서 지정된 head들을 제거.\n",
    "    attention_module: Wav2Vec2Attention 객체\n",
    "                     (encoder.layers[i].attention)\n",
    "    heads_to_prune: 리스트/집합 형태. 제거해야 할 head 번호들\n",
    "    \"\"\"\n",
    "    if not heads_to_prune:\n",
    "        return  # 제거할 head가 없으면 아무것도 안 함\n",
    "    \n",
    "    # (예) attention_module.num_heads=12, attention_module.head_dim=64\n",
    "    num_heads, head_dim = attention_module.num_heads, attention_module.head_dim\n",
    "    \n",
    "    # 이미 prune된 head가 있다면, 그 정보를 반영\n",
    "    already_pruned_heads = getattr(attention_module, \"pruned_heads\", set())\n",
    "    \n",
    "    # 제거할 head와 인덱스 계산\n",
    "    heads, index = find_pruneable_heads_and_indices(\n",
    "        heads_to_prune,\n",
    "        num_heads,\n",
    "        head_dim,\n",
    "        already_pruned_heads\n",
    "    )\n",
    "    \n",
    "    # Q, K, V, Out projection에 대해 prune\n",
    "    # 1) q_proj ( in_features -> num_heads*head_dim, out_features -> hidden_size )\n",
    "    attention_module.q_proj = prune_linear_layer(attention_module.q_proj, index, dim=1)\n",
    "    # 2) k_proj\n",
    "    attention_module.k_proj = prune_linear_layer(attention_module.k_proj, index, dim=1)\n",
    "    # 3) v_proj\n",
    "    attention_module.v_proj = prune_linear_layer(attention_module.v_proj, index, dim=1)\n",
    "    # 4) out_proj\n",
    "    #   out_proj에서 head(=channel) 방향 pruning은 weight의 \"in_features\" 차원 축소로 진행\n",
    "    attention_module.out_proj = prune_linear_layer(attention_module.out_proj, index, dim=0)\n",
    "    \n",
    "    # heads 제거 후, num_heads를 업데이트\n",
    "    attention_module.num_heads = attention_module.num_heads - len(heads)\n",
    "    # 어떤 head들이 제거되었는지 기록\n",
    "    attention_module.pruned_heads = already_pruned_heads.union(heads)\n",
    "\n",
    "\n",
    "def prune_wav2vec2_attention(model, heads_to_prune_dict):\n",
    "    \"\"\"\n",
    "    실제 Wav2Vec2Model에 대해 레이어 단위로 prune_wav2vec2_attention_layer(...) 호출.\n",
    "    \n",
    "    heads_to_prune_dict 형태 예:\n",
    "       {\n",
    "         0: [1, 2],   # layer 0에서 head 1,2 제거\n",
    "         3: [0, 5, 7] # layer 3에서 head 0,5,7 제거\n",
    "         ...\n",
    "       }\n",
    "    \"\"\"\n",
    "    for layer_idx, prune_head_list in heads_to_prune_dict.items():\n",
    "        layer_module = model.encoder.layers[layer_idx]\n",
    "        # layer_module.attention: Wav2Vec2Attention\n",
    "        prune_wav2vec2_attention_layer(layer_module.attention, prune_head_list)\n",
    "\n",
    "###\n",
    "# 클러스터링 결과로부터 layer_to_keep(유지할 head) -> heads_to_prune(제거할 head) 변환\n",
    "###\n",
    "\n",
    "def build_heads_to_prune_dict(config, layer_to_keep_dict):\n",
    "    \"\"\"\n",
    "    layer_to_keep_dict: 예) {0: [0, 2], 1: [1, 3, 5], ...}\n",
    "    config: Wav2Vec2Config\n",
    "    return: heads_to_prune 형태의 dict\n",
    "       예) { layer_i: [head_i, head_j, ...], ... }\n",
    "    \"\"\"\n",
    "    num_attention_heads = config.num_attention_heads  # (예: 12)\n",
    "    num_hidden_layers = config.num_hidden_layers      # (예: 12)\n",
    "    heads_to_prune = {}\n",
    "    \n",
    "    for layer_idx in range(num_hidden_layers):\n",
    "        keep_heads = set(layer_to_keep_dict.get(layer_idx, []))\n",
    "        all_heads = set(range(num_attention_heads))\n",
    "        # 제거해야 할 head = 전체 - 유지\n",
    "        prune_heads = sorted(list(all_heads - keep_heads))\n",
    "        if prune_heads:\n",
    "            heads_to_prune[layer_idx] = prune_heads\n",
    "    return heads_to_prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2SdpaAttention(\n",
       "  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layers[0].attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 -> num_heads:8, pruned_heads:{1, 5, 6, 7}\n",
      "Layer 1 -> num_heads:3, pruned_heads:{0, 2, 5, 6, 7, 8, 9, 10, 11}\n",
      "Layer 2 -> num_heads:2, pruned_heads:{0, 2, 3, 4, 5, 6, 8, 9, 10, 11}\n",
      "Layer 3 -> num_heads:0, pruned_heads:{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n",
      "Layer 4 -> num_heads:0, pruned_heads:{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n",
      "Layer 5 -> num_heads:0, pruned_heads:{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n",
      "Layer 6 -> num_heads:0, pruned_heads:{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n",
      "Layer 7 -> num_heads:0, pruned_heads:{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n",
      "Layer 8 -> num_heads:0, pruned_heads:{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n",
      "Layer 9 -> num_heads:1, pruned_heads:{0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n",
      "Layer 10 -> num_heads:0, pruned_heads:{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n",
      "Layer 11 -> num_heads:0, pruned_heads:{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kobie/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "### 3) 커스텀 Pruning 함수 호출\n",
    "prune_wav2vec2_attention(model, heads_to_prune)\n",
    "\n",
    "# 잘 되었는지 확인(레이어별 num_heads, pruned_heads 상태)\n",
    "for i, layer in enumerate(model.encoder.layers):\n",
    "    attn = layer.attention\n",
    "    print(f\"Layer {i} -> num_heads:{attn.num_heads}, pruned_heads:{getattr(attn, 'pruned_heads', None)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Fine-Tuning 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "from transformers import Wav2Vec2Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
      "    num_rows: 28539\n",
      "})\n",
      "Dataset({\n",
      "    features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
      "    num_rows: 2703\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# LibriSpeech ASR 데이터셋 로드 (train-clean-100 및 validation-clean)\n",
    "raw_train_dataset = load_dataset(\"Sreyan88/librispeech_asr\", \"clean\", split=\"train.100\")\n",
    "raw_eval_dataset = load_dataset(\"Sreyan88/librispeech_asr\", \"clean\", split=\"validation\")\n",
    "\n",
    "print(raw_train_dataset)\n",
    "print(raw_eval_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "    num_rows: 28539\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': '/home/kobie/.cache/huggingface/datasets/downloads/extracted/047026be8b2f50f01a897213160fbc507be2775409fe0ebdc7e040dab7eb5674/374-180298-0000.flac',\n",
       " 'audio': {'path': '/home/kobie/.cache/huggingface/datasets/downloads/extracted/047026be8b2f50f01a897213160fbc507be2775409fe0ebdc7e040dab7eb5674/374-180298-0000.flac',\n",
       "  'array': array([ 7.01904297e-04,  7.32421875e-04,  7.32421875e-04, ...,\n",
       "         -2.74658203e-04, -1.83105469e-04, -3.05175781e-05]),\n",
       "  'sampling_rate': 16000},\n",
       " 'text': 'CHAPTER SIXTEEN I MIGHT HAVE TOLD YOU OF THE BEGINNING OF THIS LIAISON IN A FEW LINES BUT I WANTED YOU TO SEE EVERY STEP BY WHICH WE CAME I TO AGREE TO WHATEVER MARGUERITE WISHED',\n",
       " 'speaker_id': 374,\n",
       " 'chapter_id': 180298,\n",
       " 'id': '374-180298-0000'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_ignore_regex = r\"[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\‘]\"\n",
    "\n",
    "def preprocess_function(batch):\n",
    "        audio = batch[\"audio\"]\n",
    "        inputs = processor([audio[\"array\"]], sampling_rate=audio[\"sampling_rate\"], return_attention_mask=True)\n",
    "        batch[\"input_values\"] = inputs.input_values[0]\n",
    "        batch[\"attention_mask\"] = inputs.attention_mask[0]\n",
    "\n",
    "        text_list = [batch[\"text\"]] if isinstance(batch[\"text\"], str) else batch[\"text\"]\n",
    "        text_encoding = processor.tokenizer(text_list)\n",
    "        batch[\"labels\"] = text_encoding.input_ids[0] if len(text_list) == 1 else text_encoding.input_ids\n",
    "        return batch\n",
    "    \n",
    "train_dataset = raw_train_dataset.map(preprocess_function, remove_columns=raw_train_dataset.column_names)\n",
    "eval_dataset = raw_eval_dataset.map(preprocess_function, remove_columns=raw_eval_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id', 'input_values', 'attention_mask', 'labels'],\n",
      "    num_rows: 28539\n",
      "})\n",
      "Dataset({\n",
      "    features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id', 'input_values', 'attention_mask', 'labels'],\n",
      "    num_rows: 2703\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = raw_train_dataset.map(\n",
    "    preprocess_function,\n",
    "    num_proc=4\n",
    ")\n",
    "eval_dataset = raw_eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "print(train_dataset)\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 43\u001b[0m\n\u001b[1;32m     39\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m labels\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m batch\n\u001b[0;32m---> 43\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorCTCWithPadding(processor\u001b[38;5;241m=\u001b[39m\u001b[43mprocessor\u001b[49m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processor' is not defined"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        # Pad audio inputs (input_values)\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Pad label inputs\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.tokenizer.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "        # Replace padding token ID with -100 for CTC loss\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch[\"input_ids\"] == self.processor.tokenizer.pad_token_id, -100\n",
    "        )\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kobie/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1964155/3332261340.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkobie9954\u001b[0m (\u001b[33mkobie9954-hanyang-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kobie/workspace/HWDHuBERT/wandb/run-20250112_214155-bzb5fg24</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kobie9954-hanyang-university/huggingface/runs/bzb5fg24' target=\"_blank\">./test-ft</a></strong> to <a href='https://wandb.ai/kobie9954-hanyang-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kobie9954-hanyang-university/huggingface' target=\"_blank\">https://wandb.ai/kobie9954-hanyang-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kobie9954-hanyang-university/huggingface/runs/bzb5fg24' target=\"_blank\">https://wandb.ai/kobie9954-hanyang-university/huggingface/runs/bzb5fg24</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 122880 at dim 1 (got 247920)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 28\u001b[0m\n\u001b[1;32m      9\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     10\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./test-ft\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# GPU 사용 시\u001b[39;00m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     21\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_ft,\n\u001b[1;32m     22\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mprocessor,\n\u001b[1;32m     26\u001b[0m )\n\u001b[0;32m---> 28\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/trainer.py:2427\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2425\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2426\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[0;32m-> 2427\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[1;32m   2429\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/trainer.py:5045\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[1;32m   5043\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m   5044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5045\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m   5046\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   5047\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/accelerate/data_loader.py:550\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/data/data_collator.py:92\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[0;34m(features, return_tensors)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# have the same attributes.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# on the whole batch.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_default_data_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/data/data_collator.py:158\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m    156\u001b[0m             batch[k] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mstack([f[k] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]))\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m             batch[k] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 122880 at dim 1 (got 247920)"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, Wav2Vec2ForCTC\n",
    "# Pruning 정보 초기화\n",
    "model.config.pruned_heads.clear()\n",
    "# model.config.pruned_heads = {}\n",
    "\n",
    "model_ft = Wav2Vec2ForCTC(config=model.config)\n",
    "model_ft.load_state_dict(model.state_dict(), strict=False)  # Pruning된 weight 적용\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test-ft\",\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=4,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,  # GPU 사용 시\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_ft,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,  # 준비된 데이터셋\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=processor,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
